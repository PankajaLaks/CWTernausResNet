{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjF9vthzqxcDPEMOeu3qJx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PankajaLaks/CWTernausResNet/blob/main/CWTernausResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8pCGH3GcvWz"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "ResNet50-TernausNet classifier (fixed):\n",
        "- Deterministic stratified 80/10/10 split\n",
        "- Consistent preprocessing for train/val/test and single-image inference\n",
        "- Class weights to mitigate imbalance\n",
        "- ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "- Diagnostics: class counts, sample counts, confusion matrix + report\n",
        "Requirements: tensorflow>=2.10, numpy, scikit-learn, matplotlib\n",
        "\"\"\"\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import itertools\n",
        "\n",
        "# ==============================\n",
        "# Config\n",
        "# ==============================\n",
        "DATA_DIR = \"/content/drive/MyDrive/DS\"   # <-- folder containing subfolders for each class\n",
        "INPUT_SHAPE = (256, 256, 3)\n",
        "BATCH_SIZE  = 16\n",
        "EPOCHS      = 30\n",
        "LR          = 1e-3\n",
        "SEED        = 42\n",
        "RESNET_WEIGHTS = \"imagenet\"   # or None\n",
        "DROPOUT_RATE    = 0.25\n",
        "FEATURE_STAGE   = \"decoder\"   # 'decoder' or 'bottleneck'\n",
        "FEATURE_CHANNELS = 64\n",
        "\n",
        "tf.keras.utils.set_random_seed(SEED)\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# ==============================\n",
        "# Utilities: dataset creation (stratified splits)\n",
        "# ==============================\n",
        "def gather_image_paths_and_labels(data_dir, exts=(\".jpg\",\".jpeg\",\".png\",\".bmp\")):\n",
        "    data_dir = Path(data_dir)\n",
        "    class_dirs = [d for d in sorted(data_dir.iterdir()) if d.is_dir()]\n",
        "    class_names = [d.name for d in class_dirs]\n",
        "    paths = []\n",
        "    labels = []\n",
        "    for idx, d in enumerate(class_dirs):\n",
        "        for ext in exts:\n",
        "            for p in d.rglob(f\"*{ext}\"):\n",
        "                paths.append(str(p))\n",
        "                labels.append(idx)\n",
        "    paths = np.array(paths)\n",
        "    labels = np.array(labels)\n",
        "    # sort by path for determinism (optional), then shuffle via train_test_split with seed\n",
        "    order = np.argsort(paths)\n",
        "    return paths[order], labels[order], class_names\n",
        "\n",
        "def decode_and_resize(path, label, img_size=INPUT_SHAPE[:2], augment=False):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)  # [0,1]\n",
        "    img = tf.image.resize(img, img_size)\n",
        "    if augment:\n",
        "        img = tf.image.random_flip_left_right(img)\n",
        "        img = tf.image.random_brightness(img, max_delta=0.05)\n",
        "        # small random rotation\n",
        "        angle = tf.random.uniform([], -0.03, 0.03)\n",
        "        img = tfa.image.rotate(img, angle) if \"tfa\" in globals() else img  # optional if tensorflow_addons installed\n",
        "    return img, label\n",
        "\n",
        "def make_dataset(paths, labels, batch_size=BATCH_SIZE, shuffle=False, augment=False):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(paths), seed=SEED, reshuffle_each_iteration=True)\n",
        "    ds = ds.map(lambda p, l: decode_and_resize(p, l, augment=augment), num_parallel_calls=AUTOTUNE)\n",
        "    ds = ds.batch(batch_size).prefetch(AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "# ==============================\n",
        "# Build model (ResNet50 + Ternaus-style decoder -> GAP -> classifier)\n",
        "# ==============================\n",
        "def conv_block(x, filters, name=None):\n",
        "    x = layers.Conv2D(filters, 3, padding=\"same\", use_bias=False, name=None if not name else name+\"_conv1\")(x)\n",
        "    x = layers.BatchNormalization(name=None if not name else name+\"_bn1\")(x)\n",
        "    x = layers.Activation(\"relu\", name=None if not name else name+\"_relu1\")(x)\n",
        "    x = layers.Conv2D(filters, 3, padding=\"same\", use_bias=False, name=None if not name else name+\"_conv2\")(x)\n",
        "    x = layers.BatchNormalization(name=None if not name else name+\"_bn2\")(x)\n",
        "    x = layers.Activation(\"relu\", name=None if not name else name+\"_relu2\")(x)\n",
        "    return x\n",
        "\n",
        "def up_block(x, skip, filters, name=None):\n",
        "    x = layers.UpSampling2D((2, 2), interpolation=\"bilinear\", name=None if not name else name+\"_up\")(x)\n",
        "    x = layers.Concatenate(name=None if not name else name+\"_concat\")([x, skip])\n",
        "    x = conv_block(x, filters, name=None if not name else name+\"_convblock\")\n",
        "    return x\n",
        "\n",
        "def build_resnet50_feature_extractor(input_shape=INPUT_SHAPE, encoder_weights=\"imagenet\",\n",
        "                                     feature_stage=\"decoder\", feature_channels=FEATURE_CHANNELS):\n",
        "    assert feature_stage in (\"decoder\", \"bottleneck\")\n",
        "    base = ResNet50(include_top=False, weights=encoder_weights, input_shape=input_shape)\n",
        "    inputs = base.input\n",
        "    c1 = base.get_layer(\"conv1_relu\").output\n",
        "    c2 = base.get_layer(\"conv2_block3_out\").output\n",
        "    c3 = base.get_layer(\"conv3_block4_out\").output\n",
        "    c4 = base.get_layer(\"conv4_block6_out\").output\n",
        "    c5 = base.get_layer(\"conv5_block3_out\").output\n",
        "\n",
        "    # bottleneck convs\n",
        "    x = layers.Conv2D(1024, 3, padding=\"same\", use_bias=False, name=\"bottleneck_conv1\")(c5)\n",
        "    x = layers.BatchNormalization(name=\"bottleneck_bn1\")(x)\n",
        "    x = layers.Activation(\"relu\", name=\"bottleneck_relu1\")(x)\n",
        "    x = layers.Conv2D(1024, 3, padding=\"same\", use_bias=False, name=\"bottleneck_conv2\")(x)\n",
        "    x = layers.BatchNormalization(name=\"bottleneck_bn2\")(x)\n",
        "    x = layers.Activation(\"relu\", name=\"bottleneck_relu2\")(x)\n",
        "\n",
        "    if feature_stage == \"bottleneck\":\n",
        "        return models.Model(inputs, x, name=\"ResNet50_Ternaus_Bottleneck\")\n",
        "\n",
        "    # decoder\n",
        "    d4 = up_block(x, c4, 512, name=\"dec4\")\n",
        "    d3 = up_block(d4, c3, 256, name=\"dec3\")\n",
        "    d2 = up_block(d3, c2, 128, name=\"dec2\")\n",
        "    d1 = up_block(d2, c1, 64, name=\"dec1\")\n",
        "    d0 = layers.UpSampling2D((2, 2), interpolation=\"bilinear\", name=\"dec0_up\")(d1)\n",
        "    d0 = conv_block(d0, 64, name=\"dec0_convblock\")\n",
        "    feats = layers.Conv2D(feature_channels, 3, padding=\"same\", activation=\"relu\", name=\"features_conv\")(d0)\n",
        "    return models.Model(inputs, feats, name=\"ResNet50_Ternaus_Features\")\n",
        "\n",
        "def build_resnet50_ternaus_classifier(input_shape=INPUT_SHAPE, num_classes=3, encoder_weights=\"imagenet\",\n",
        "                                      feature_stage=\"decoder\", feature_channels=64, dropout_rate=0.25):\n",
        "    ternaus = build_resnet50_feature_extractor(input_shape, encoder_weights, feature_stage, feature_channels)\n",
        "    x_in = ternaus.input\n",
        "    feats = ternaus.output\n",
        "    x = layers.GlobalAveragePooling2D(name=\"gap\")(feats)\n",
        "    if dropout_rate and dropout_rate > 0:\n",
        "        x = layers.Dropout(dropout_rate, name=\"cls_dropout\")(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"classifier\")(x)\n",
        "    return models.Model(x_in, outputs, name=\"ResNet50_Ternaus_Classifier\")\n",
        "\n",
        "# ==============================\n",
        "# Evaluation helper\n",
        "# ==============================\n",
        "def evaluate_and_print(model, dataset, class_names):\n",
        "    y_true = []\n",
        "    for _, y in dataset.unbatch().batch(1024):  # gather in larger chunks\n",
        "        y_true.append(y.numpy())\n",
        "    y_true = np.concatenate(y_true)\n",
        "    y_proba = model.predict(dataset, verbose=0)\n",
        "    y_pred = np.argmax(y_proba, axis=1)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "    acc = (y_true == y_pred).mean()\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    print(f\"Accuracy: {acc:.4f} | Macro Precision: {prec:.4f} | Macro Recall: {rec:.4f} | Macro F1: {f1:.4f}\")\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title(\"Confusion matrix\")\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names, rotation=45)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ==============================\n",
        "# Single image inference\n",
        "# ==============================\n",
        "def preprocess_image_path(img_path, img_size=INPUT_SHAPE[:2]):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    img = tf.image.resize(img, img_size)\n",
        "    x = tf.expand_dims(img, axis=0)\n",
        "    return x.numpy()\n",
        "\n",
        "def predict_single_image(model, img_path, class_names):\n",
        "    x = preprocess_image_path(img_path)\n",
        "    proba = model.predict(x, verbose=0)[0]\n",
        "    idx = int(np.argmax(proba))\n",
        "    print(f\"\\nImage: {img_path}\")\n",
        "    for i, cname in enumerate(class_names):\n",
        "        print(f\"  {cname}: {proba[i]:.4f}\")\n",
        "    print(f\"Predicted: {class_names[idx]} (index={idx})\")\n",
        "    return idx, proba\n",
        "\n",
        "# ==============================\n",
        "# Main\n",
        "# ==============================\n",
        "def main():\n",
        "    # Gather paths + labels\n",
        "    paths, labels, class_names = gather_image_paths_and_labels(DATA_DIR)\n",
        "    assert len(class_names) >= 2, \"Need at least 2 classes in subfolders.\"\n",
        "    print(\"Detected classes (alphabetical):\", class_names)\n",
        "\n",
        "    # Stratified split: train 80%, pool 20% -> then pool split 50/50 for val/test (10/10)\n",
        "    p_train, p_pool, y_train, y_pool = train_test_split(paths, labels, test_size=0.2, stratify=labels, random_state=SEED)\n",
        "    p_val, p_test, y_val, y_test = train_test_split(p_pool, y_pool, test_size=0.5, stratify=y_pool, random_state=SEED)\n",
        "\n",
        "    print(f\"Samples: total={len(paths)}, train={len(p_train)}, val={len(p_val)}, test={len(p_test)}\")\n",
        "    print(\"Class distribution in training set:\", np.bincount(y_train))\n",
        "\n",
        "    # Build tf.data datasets with identical preprocessing\n",
        "    # (augmentation only on train)\n",
        "    try:\n",
        "        # If tensorflow_addons available, we used above in decode_and_resize - otherwise it's gracefully skipped\n",
        "        import tensorflow_addons as tfa  # noqa: F401\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    train_ds = make_dataset(p_train, y_train, batch_size=BATCH_SIZE, shuffle=True, augment=True)\n",
        "    val_ds   = make_dataset(p_val,   y_val,   batch_size=BATCH_SIZE, shuffle=False, augment=False)\n",
        "    test_ds  = make_dataset(p_test,  y_test,  batch_size=BATCH_SIZE, shuffle=False, augment=False)\n",
        "\n",
        "    # Class weights\n",
        "    classes = np.unique(y_train)\n",
        "    cw = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
        "    class_weight = {int(c): float(w) for c, w in zip(classes, cw)}\n",
        "    print(\"Class weights:\", class_weight)\n",
        "\n",
        "    # Build & compile model\n",
        "    num_classes = len(class_names)\n",
        "    model = build_resnet50_ternaus_classifier(\n",
        "        input_shape=INPUT_SHAPE,\n",
        "        num_classes=num_classes,\n",
        "        encoder_weights=RESNET_WEIGHTS,\n",
        "        feature_stage=FEATURE_STAGE,\n",
        "        feature_channels=FEATURE_CHANNELS,\n",
        "        dropout_rate=DROPOUT_RATE\n",
        "    )\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(LR),\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    model.summary()\n",
        "\n",
        "    # Callbacks\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.ModelCheckpoint(\"best_resnet_ternaus.keras\", monitor=\"val_loss\", save_best_only=True),\n",
        "        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=7, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
        "    ]\n",
        "\n",
        "    # Train\n",
        "    history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS,\n",
        "                        callbacks=callbacks, class_weight=class_weight)\n",
        "\n",
        "    # Evaluate\n",
        "    print(\"\\nEvaluating on test setâ€¦\")\n",
        "    test_loss, test_acc = model.evaluate(test_ds, verbose=0)\n",
        "    print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
        "    evaluate_and_print(model, test_ds, class_names)\n",
        "\n",
        "    # Example single image inference (change path as needed)\n",
        "    example_image = \"/content/drive/MyDrive/NORMAL2-IM-1442-0001.jpeg\"\n",
        "    predict_single_image(model, example_image, class_names)\n",
        "\n",
        "    model.save(\"/content/drive/MyDrive/res_ternausnet.keras\")\n",
        "    model.save(\"/content/drive/MyDrive/res_ternausnet.h5\")\n",
        "    print(\"Saved model to /content/drive/MyDrive/res_ternausnet.keras\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}